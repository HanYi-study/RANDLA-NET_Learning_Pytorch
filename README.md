# RANDLA-NET_Learning_Pytorch
原github网址：https://github.com/liuxuexun/RandLA-Net-Pytorch-New/tree/master

# RandLA-NET-PYTORCH-NEW 项目目录结构

```
RandLA-Net-Pytorch-New/
├── __pycache__/                        # Python 编译缓存文件夹
│   ├── helper_ply.cpython-37.pyc       # helper_ply.py 的编译缓存
│   ├── helper_tool.cpython-37.pyc      # helper_tool.py 的编译缓存
│   ├── pytorch_utils.cpython-37.pyc    # pytorch_utils.py 的编译缓存
│   ├── RandLANet.cpython-37.pyc        # RandLANet.py 的编译缓存
│   └── s3dis_dataset.cpython-37.pyc    # s3dis_dataset.py 的编译缓存
│
├── .idea/                              # PyCharm/IDEA 工程配置文件夹
│   ├── inspectionProfiles/             # 代码检查配置
│   ├── misc.xml                        # 工程配置
│   ├── modules.xml                     # 模块配置
│   ├── RandLA-Net-Pytorch-New.iml      # 工程模块文件
│   ├── vcs.xml                         # 版本控制配置
│   └── workspace.xml                   # 工作区配置
│
├── data/                               # 数据目录
│   ├── input_0.040/                    # 下采样后点云、KDTree、投影索引等
│   ├── original_ply/                   # 合并后的原始点云 ply 文件
│   └── Stanford3dDataset_v1.2_Aligned_Version/ # S3DIS 原始数据集
│
├── previous_license/                   # 历史许可证和说明文档
│   ├── LICENSE                         # 旧版许可证
│   └── README.md                       # 说明文档
│
├── test_output/                        # 推理/测试输出目录
│   ├── 2025-08-03_08-57-08/            # 按时间戳分文件夹保存本次测试结果
│   │   └── val_preds/                  # 验证集预测结果
│   │       └── log_test_Area_5.txt     # 测试日志
│
├── train_output/                       # 训练输出目录
│   └── 2025-07-12_01-48-28/            # 按时间戳分文件夹保存本次训练结果
│       ├── checkpoint.tar              # 训练断点模型权重
│       └── log_train_Area_5.txt        # 训练日志
│
├── utils/                              # 工具与数据处理模块
│   ├── cpp_wrappers/                   # C++/CUDA 加速模块源码
│   ├── meta/                           # 元数据（如类别名、路径等）
│   ├── nearest_neighbors/              # 最近邻查找相关 C++/Python 实现
│   │   ├── knn.cpp / knn.h / knn.o     # KNN 算法源码及编译文件
│   │   ├── KDTreeTableAdaptor.h        # KDTree 适配器头文件
│   │   ├── nanoflann.hpp               # nanoflann KDTree 库
│   │   ├── knn.pyx                     # Cython 封装
│   │   ├── knn_.cxx / knn_.h           # 生成的 C++ 文件
│   │   ├── setup.py                    # 编译脚本
│   │   ├── test.py                     # 测试脚本
│   │   └── lib/python/                 # 编译生成的 Python 库
│   ├── 6_fold_cv.py                    # S3DIS 六折交叉验证脚本
│   ├── data_prepare_s3dis.py           # S3DIS 数据预处理脚本
│   ├── data_prepare_semantic3d.py      # Semantic3D 数据预处理脚本
│   ├── data_prepare_semantickitti.py   # SemanticKITTI 数据预处理脚本
│   ├── download_semantic3d.sh          # 下载 Semantic3D 数据集脚本
│   ├── semantic-kitti.yaml             # SemanticKITTI 标签映射配置
│
├── compile_op.sh                       # CUDA/C++ 加速模块编译脚本
├── cuda_11.6.0_510.39.01_linux.run     # CUDA 安装包（仅供环境搭建参考）
├── helper_ply.py                       # PLY 点云文件读写工具
├── helper_tool.py                      # 常用数据处理与配置工具
├── job_for_testing.sh                  # Shell 脚本：批量模型测试任务
├── LICENSE                             # 项目许可证
├── main_S3DIS.py                       # S3DIS 数据集训练主程序
├── main_SemanticKITTI.py               # SemanticKITTI 数据集训练主程序
├── nvidia-compute-utils-550_xxx.deb    # NVIDIA 驱动相关安装包（仅供环境搭建参考）
├── pytorch_utils.py                    # PyTorch 相关辅助工具
├── RandLANet.py                        # RandLA-Net 网络结构与训练核心
├── README.md                           # 项目说明文档
├── s3dis_dataset.py                    # S3DIS 数据集 Dataset 类
├── semantic_kitti_dataset.py           # SemanticKITTI 数据集 Dataset 类
├── test_S3DIS.py                       # S3DIS 数据集测试/推理脚本
└── test_SemanticKITTI.py               # SemanticKITTI 数据集测试/推
```
08/04创建  
08/10更新  
不包含学习代码用的各.md文件

---

# RandLA-Net (S3DIS) 完整运行流程总结

本流程以 S3DIS 数据集为例，详细说明 RandLA-Net 从数据预处理到训练、测试的每一步，涉及的文件、输入输出数据及操作内容。

---

## 1. 数据预处理

### 1.1 负责文件
- `utils/data_prepare_s3dis.py`

### 1.2 主要操作
- 1）读取 S3DIS 原始数据集（`data/Stanford3dDataset_v1.2_Aligned_Version/`），每个房间一个文件夹，内含多个实例的 txt 文件（XYZRGB，没有标签，文件名或者某种方式可以区分不同实例）。
- 2）合并每个房间所有实例的 txt 文件，生成带标签的点云（XYZRGBL）：遍历每个房间文件夹，收集所有实例txt文件。读取每个实例txt文件，添加**标签列**，存储合并后的点云
- 3）对合并后的点云做**网格下采样**（如 0.04m）：
    - @ 首先将每个房间的所有txt读取合并为**N×7矩阵：xyz rgb label**并保存为ply文件；**原始数据合并后得到的是N×7维度(x,y,z,r,g,b,label)的.ply文件（保存在original_ply/*.ply），用于原始点云存档**
        - 为什么原始点云数据合并需要保留N×7维度的信息？
            - 首先是原始数据备份： original_ply/*.ply保留完整的原始信息（XYZRGB+label），用于调试或后续可能的重新处理。
            - 标签投影需要：proj.pkl中的标签来源于原始点云，需确保与下采样点的空间对应关系正确。
    - @ 调用DP.grid_sub_sampling()函数（DP为utils.tf_ops或utils.cpp_wrappers下的点云处理库）实现网格采样；
        - 空间网格划分：
            - 首先，将整个点云空间按照0.04米为边长划分为一个个立方体网格（体素，cube/grid）
            - 每个点 (x, y, z) 通过除以 0.04 并向下取整（floor），落入某一个网格单元。
        - 网格内点聚合：
            - 对每个网格，统计所有落入该网格的点。
            - 常见聚合方式：取网格内所有点的坐标均值（质心）作为代表点
            - 对应的颜色、标签也通常采用均值或多数投票
        - 输出下采样点云：
            - 每个有点的网格只输出一个代表点（大大减少点数，稀疏化点云）
    - @ 下采样结果保存到input_0.040目录下。**网格下采样后得到的是N×6维度(x,y,z,r,g,b)的.ply文件（保存在input_0.040/*.ply），用于存储下采样点云**
        - 为什么下采样后ply是N×6？
            - 点云与标签分离：RandLA-Net将几何信息（xyz+rgb）和语义标签分开存储，input_0.040/* .ply仅保存点的几何和颜色特征（用于网络输入），input_0.040/*_proj.pkl保存原始标签及投影关系（用于训练时动态分配标签）
            - KDTree只需要坐标（xyz）
            - 标签一致性：标签通过proj.pkl中的proj_inds动态映射到下采样点，避免因下采样导致的标签歧义
- 4）建立 **KDTree** 并保存为 pkl：
    - 首先使用上一步生成的input_0.040/* .ply点云（N×6矩阵，包含xyzrgb）/ 经过了下采样的点云
    - 其次进行KDTree的构建：提取点云坐标xyz（忽略rgb），使用sklearn.neighbors.KDTree构建空间索引结构。
    - 然后是保存KDTree，使用pickle序列化KDTree对象，保存为*_KDTree.pkl文件。
    - 最后是关联投影索引，同时生成*_proj.pkl，记录原始点到下采样点的最近邻索引（利用KDTree加速查询）。------------------------------第一次投影索引
    - 经过以上处理后输出了```input_0.040/*_KDTree.pkl```（KDTree对象文件，支持高效KNN查询，包含了KDTree索引，用于加速邻域查询）和```input_0.040/*_proj.pkl```（投影索引文件，内容包含了投影索引+原始标签，用于表爱你动态分配）
- 5）保存原始点到下采样点的最近邻投影索引和标签为 pkl。
    - 首先输入数据：原始点云（original_ply/* .ply，N×7，包含 x y z r g b label）和下采样点云（input_0.040/*.ply，M×6，仅 x y z r g b）
    - 计算最近邻投影索引：使用 KDTree（基于下采样点云构建）查询每个原始点的最近邻下采样点索引---------------------------------------第二次投影索引
    - 保存标签和投影索引：原始标签直接取自original_ply的第七列，然后二者存储为.pkl文件。
- **投影索引**：
    - 什么是投影索引：投影索引（Projection Index）是一个一维数组，记录原始点云中每个点对应的下采样点云中的最近邻点的索引。
    - 形状：[N_original]（N_original = 原始点云点数）
    - 内容：每个元素是下采样点云中的点的索引（如proj_inds[0] = 3表示原始点0映射到下采样点3）。
    - 为什么需要投影索引：解决下采样导致的标签与点云不对齐问题。
        - 下采样会丢失点：网格下采样后，多个原始点可能合并为一个下采样点，导致原始标签无法直接使用。
        - 标签传递：通过投影索引，将原始标签正确分配给下采样点（如多数投票）。
        - 预测还原：测试时需将下采样点的预测结果插值回原始点云（依赖投影关系）。
    - 如何计算投影索引
        - 构建下采样点的KDTree
        - 查询原始点的最近邻下采样点
        - 保存为proj.pkl
        - ```text
          原始点云: A(0), B(1), C(2), D(3)      标签: [0, 1, 2, 3]
          下采样点云: X, Y, Z                     (A,B→X; C→Y; D→Z)
          投影索引: [0, 0, 1, 2]                (A→X, B→X, C→Y, D→Z)
          ```
    - 为什么两次计算最近邻投影索引？
        - 第一次投影索引（用于训练时的标签分配）：下采样后生成input_0.040/*_proj.pkl时进行计算 / 从原始点云，进行下采样生成下采样点时 / 输出内容：proj_inds, labels / 用途：训练时动态分配标签（如多数投票）
        - 第二次投影索引（用于测试时的预测还原）：测试时加载input_0.040/*_proj.pkl / 从下采样点反向推得原始点云 / 输出内容：下采样点预测结果插值到原始点，或者说将网络输出的下采样点预测结果扩展回原始分辨率 / 用途：测试阶段将预测结果还原到原始点云（可视化或计算mIoU）

### 1.3 输入数据
- `data/Stanford3dDataset_v1.2_Aligned_Version/Area_x/room_y/Annotations/*.txt`

### 1.4 输出数据
- `data/original_ply/*.ply`：合并后的原始点云（XYZRGBL）
- `data/input_0.040/*.ply`：下采样点云（XYZRGB）
- `data/input_0.040/*_KDTree.pkl`：KDTree 对象，包含了KDTree索引，支持KNN高效查询
- `data/input_0.040/*_proj.pkl`：原始点到下采样点的投影索引和标签

---

## 2. 数据集加载与采样

### 2.1 负责文件
- `s3dis_dataset.py`
- `helper_tool.py`（数据处理与采样工具）

### 2.2 主要操作
- **数据集加载**：通过 `S3DIS` 类加载预处理后的点云、标签、KDTree、投影索引等。
    - 首先读取文件：从存储的数据文件（如下采样后的.ply或.npy文件）中读取点云数据，包括点的坐标（xyz）、颜色（rgb）、标签（L）等
    - 然后统一数据结构：通常会将每个房间的点云数据加载到内存，构建为统一的numpy数组或pytorch tensor，便于后续批处理
        - 流程：
            - 1）首先读取每个房间下所有实例的txt文件，内容格式为XYZRGB，用pandas读取为numpy数组，并给每个实力分配标签
            - 2）合并所有点形成统一矩阵，将data_list中所有实例的点合并成一个numpy数据（大矩阵），shape为（总点数，7）
            - 3）坐标归一化/偏移：  
                      **取xyz的最小值**（在一个房间的点云中，xyz最小值代表这个房间点云的最靠近远点的角落），**所有点坐标减去最小值**（这样做的本质是把整个点云平移，让坐标系的起点成房间点云的最小点，保证所有点的xyz都是大于等于0 的，从而可以消除全局坐标系带来的大数值和偏移影响）（例如，假如原始点云有坐标范围 [1012.4, 1023.6]，经过归一化后变成 [0, 11.2]，大大缩小了数值范围），**实现坐标归一化**（归一化就是把数据变换到某个标准范围，例如数值点都落在[0,1]内，准确的说是坐标偏移，即所有点减去最小坐标，把所有点云坐标平移到以最小点云坐标为坐标原点的坐标系中，归一化后点云的最小坐标就变成了（0，0，0），有些情况下归一化还会除以最大值或者范围，从而实现标准化到[0,1]，但是randlanet的预处理只是坐标的偏移）
        - 统一数据结构的结果：
          ```text
          [[x1, y1, z1, r1, g1, b1, label1],
          [x2, y2, z2, r2, g2, b2, label2],
          ...
          ]
          ```
          例如：
          ```text
          [[ 0.000, 1.123, 2.324, 120, 130, 125,  1],
           [ 0.020, 1.121, 2.310, 124, 131, 126,  2],
           [ 0.040, 1.129, 2.307, 121, 132, 127,  1]]
          ```
    - 最后进行索引与映射：还会加载KDTree、点的索引映射等辅助数据，用于快速空间搜索和数据增强
- **采样/空间均匀采样**：通过 `S3DISSampler` 实现空间均匀采样（spatially regular sampling），每次采样一批点用于训练/验证。
    - **@ 采用概率机制，每次从点云中抽取一个中心点及其邻域，保证采样点云块在空间上的均匀分布，增强训练泛化。** 
        - 什么是概率机制：
            - 基本思路：每次采样时，不是简单随机抽取一个点，而是根据“概率表”或“可能性分布”选取采样的中心点
            - 实现方法：
                - 1）为每个点设置一个采样的“可能性值”，所有点初始值是一样的（通常很小的正数）
                - 2）每采样一次，选中可能性值最小的点作为中心点（即“被采样得最少的区域”）
                - 3）对以该点为中心的邻域所有点，其可能性值都加上采样距离的倒数（采样块内点离中心近的加的多，远的加的少 / 这就是为什么选择加倒数）
        - 每次都是随机抽取一个点嘛：不完全随机，而是优先选择可能性值最小的点。这样做可以保证优先采样未覆盖的区域，实现空间均匀采样。
            - 第一个采样点是随机采样的吗：通常第一个采样点是完全随机的。（采样刚开始的时候，所有点的可能性值都是一样的，都是最小值）
            - 那假设点云中一共有100个点，那前100个采样点是都是随机采样的吗？（答：不是，只有第一个采样点是完全随机的，之后的采样就开始依赖可能性值机制）或者说刚开始采样的时候大部分可能性值都是最小值（假设为0），同时存在50个可能性值为0 的被采样点时，如何选择采样哪一个点呢？（答：在多个可能性值最小的点中，随机选择一个）
        - 什么时候抽样结束：采样会不断进行，直到一个epoch内“所有点的可能性值都被采样到一定阈值以上”，但是实际上通常是根据epoch需要的采样块数（例如一个epoch采样N个块），而不是必须覆盖所有点。
            - 如何定义一个epoch？（一个epoch通常是遍历一次训练集的所有采样块，这里的采样块不是所有点，而是每次以空间采样机制截取的局部点云块，epoch的长度就是训练集中采样块的总数）一个epoch中需要采样多少个点如何知道的？（由训练配置参数决定，每个采样块通常包含固定数量的点，可以通过KNN或ball query获得）可能性值的阈值是多少？（答：采样终止条件是猜满指定数量的采样块，可能性值的阈值通常用作调试或特殊用途，并非主流终止条件）
        - 为什么抽取一个点还需要抽取其邻域：采样中心只是一个参考位置，实际训练用的是一个“采样块”（以这个点为中心，包含其邻域的若干点，比如KNN或ball query，常设置40960个点）这样每个采样块是一个局部空间片段，输入给神经网络做分割训练。采样中心点的邻域保证了 块内有空间关联的信息，有利于空间特征学习。
    - **@ 每采样一次，提升该区域的采样概率，减少重复采样。（使用了概率机制，从而实现空间点均匀采样）** 
    - **@ 获取中心点周围的邻域点组成采样块，常用 KNN 或 Ball Query 方法。** 
    - **@ 采样流程：** 
        - **1）选择中心点（带扰动）**
            - 在指定完中心点后，对其坐标加上一个很小的高斯噪声（扰动）
            - 带扰动是因为防止多次采样时中心点完全重合（尤其是在概率机制下，有些区域可能性值最小值相同），增强局部空间多样性，提高模型对空间细节的泛化能力，可以避免采样块之间重复、边界硬分割，减少训练时的“死角”。
        - **2）搜索邻域点（KNN/Ball Query，固定数量）（2种邻域搜索策略）**
            - KNN（K-Nearest Neighbors）：指定固定数量的邻域点，由k值来确定，获取最近k个点
            - ball query（球查询）：固定半径r，选择落在以中心点为球心，半径r的球内的所有点。可以实现采样块空间范围恒定，点数不定
            - randlanet默认使用knn，保证每个采样块的点数一致。
        - **3）块中心化、特征提取、标签获取**
            - 块中心化：将采样块内所有点的xyz坐标减去中心点的xyz（数据集加载过程中，对点云数据进行归一化处理，其中减数是最小的点云坐标值，坐标系的原点是最小值坐标点的xyz），使采样块空间居中，中心点的坐标变为（0，0，0）。从而提升模型对相对空间结构的建模能力，消除全局坐标的影响，增强空间不变性。
            - 特征提取：获取采样块内所有点的特征（如rgb、强度、法向量），通常直接切片，共神经网络输入使用。为网络提供“点的描述”，便于学习空间-语义关联。
                - 为什么要切片？（答：切片指定是用numpy或pytorch的索引的方式，从整个房间的点云大数组中，取出采样块内所有点的特征。采样块只需要自己块内的点，因此要用索引把整个房间的特征、标签、坐标提取出采样块的内容）切的是谁？答：切的是“特征矩阵”或者“点云属性矩阵”，例如features是整个房间所有点的特征，shape为[N,3]（N是点的数量，每行代表一个点，3 则代表只包含了rgb）labels是整个房间所有点的标签[N,1]，points是整个房间所有点的坐标[N,3]。
                - - 切片前长什么样？
                ```python
                 features = [
                 [255, 0, 0],     # 点0，红色
                 [0, 255, 0],     # 点1，绿色
                 [0, 0, 255],     # 点2，蓝色
                 [128, 128, 128], # 点3，灰色
                 [255, 255, 0],   # 点4，黄色
                  ]  # shape (5, 3)

                 points = [
                 [0.1, 0.2, 0.3], # 点0的位置
                 [1.0, 0.5, 0.3], # 点1的位置
                 [0.2, 0.1, 0.4], # 点2的位置
                 [3.2, 1.2, 0.8], # 点3的位置
                 [2.1, 0.5, 0.6], # 点4的位置
                 ]  # shape (5, 3)

                 labels = [
                 [1],  # 点0，类别1
                 [2],  # 点1，类别2
                 [1],  # 点2，类别1
                 [3],  # 点3，类别3
                 [2],  # 点4，类别2
                  ]  # shape (5, 1)
                 ```
                - 如何切片的？
                ```python
                block_features = features[queried_idx]
                block_points   = points[queried_idx]
                block_labels   = labels[queried_idx]
                ```
                用queried_idx对全房间数据做切片，得到采样块
                - 切片后的数据长什么样？ 假设采样时依次选择了 点3、点1、点4 ： queried_idx = [3, 1, 4] 
                ```python
                block_features = [
                [128, 128, 128], # 点3，灰色
                [0, 255, 0],     # 点1，绿色
                [255, 255, 0],   # 点4，黄色
                ]  # shape (3, 3)

                block_points = [
                [3.2, 1.2, 0.8], # 点3的位置
                [1.0, 0.5, 0.3], # 点1的位置
                [2.1, 0.5, 0.6], # 点4的位置
                ]  # shape (3, 3)

                block_labels = [
                [3],  # 点3，类别3
                [2],  # 点1，类别2
                [2],  # 点4，类别2
                ]  # shape (3, 1)
                ```
                切片前是整个房间的所有点（5个点），切片后只剩下采样块内的点（3个点），用于神经网络训练，切片就是用queried_idx从全体中“抽出”你要的那些点。
                - 为何可以便利空间-语义关联的学习？什么是空间-语义关联？（答：**空间-语义关联**指的是模型不仅能理解点的空间位置关系（谁在谁旁边），还能结合点的属性（例如颜色、强度、法向量等）去推断其语义类别（例如是地面？墙？桌子...）。切片提取采样块的特征，让模型可以输入局部空间的所有点的空间坐标+属性，这样网络可以感知点云在空间上的结构关系（如几何形状）、利用点的属性（如颜色)作为语义判别的线索、通过空间和属性的联合特征学习到“什么位置、什么属性对应什么语义类别”，从而实现空间-语义的结合
            - 标签获取：获取采样块内所有点的标签（如语义类别、实例id），也通常是直接切片，训练时作为监督信号。监督训练，计算损失。
                - 什么叫监督信号？监督信号如何发挥作用？标签获取过程中什么操作可以计算损失？

### 2.3 输入数据
- `data/input_0.040/*.ply`
- `data/input_0.040/*_KDTree.pkl`
- `data/input_0.040/*_proj.pkl`

### 2.4 输出数据
- 采样得到的点云 batch（点坐标、颜色、标签、索引等），供 DataLoader 使用。

---

## 3. 数据加载与批处理

### 3.1 负责文件
- `main_S3DIS.py`
- `s3dis_dataset.py`（`collate_fn`）

### 3.2 主要操作
- 使用 PyTorch `DataLoader` 加载采样后的 batch 数据。
- `collate_fn` 组装多层 KNN 索引、下采样索引、上采样索引等，形成网络输入格式。

### 3.3 输入数据
- 采样得到的点云 batch

### 3.4 输出数据
- 组装好的 batch 字典（包含 `xyz`, `features`, `labels`, `neigh_idx`, `sub_idx`, `interp_idx`, `input_inds`, `cloud_inds` 等）

---

## 4. 模型定义与训练

### 4.1 负责文件
- `RandLANet.py`（模型结构、损失、评估等）
- `main_S3DIS.py`（训练主程序）

### 4.2 主要操作
- 构建 RandLA-Net 网络结构。
- 前向传播，计算损失（`compute_loss`）、准确率（`compute_acc`）、IoU（`IoUCalculator`）。
- 反向传播与参数更新。
- 日志记录与模型保存。

### 4.3 输入数据
- DataLoader 输出的 batch 字典

### 4.4 输出数据
- 训练日志（如 `train_output/2025-07-12_01-48-28/log_train_Area_5.txt`）
- 断点模型权重（如 `train_output/2025-07-12_01-48-28/checkpoint.tar`）

---

## 5. 验证与测试

### 5.1 负责文件
- `test_S3DIS.py`
- `RandLANet.py`
- `s3dis_dataset.py`

### 5.2 主要操作
- 加载训练好的模型权重。
- 对验证/测试集进行推理，支持多次投票平滑预测。
- 将下采样点云的预测结果投影回原始点云。
- 计算混淆矩阵、IoU、准确率等指标。
- 保存预测结果（ply 文件）和测试日志。

### 5.3 输入数据
- 训练好的模型权重
- 预处理后的测试集数据（同第2步）

### 5.4 输出数据
- 测试日志（如 `test_output/2025-08-03_08-57-08/val_preds/log_test_Area_5.txt`）
- 预测结果 ply 文件（如 `test_output/2025-08-03_08-57-08/val_preds/*.ply`）

---

## 6. 可选：交叉验证与批量测试

### 6.1 负责文件
- `utils/6_fold_cv.py`（六折交叉验证）
- `job_for_testing.sh`（批量测试脚本）

### 6.2 主要操作
- 自动化多 Area 交叉验证训练与测试
- 批量提交测试任务

---

# 总结流程图

1. **数据预处理**  
   - `utils/data_prepare_s3dis.py`  
   - 输入：原始 txt  
   - 输出：ply、KDTree、proj.pkl

2. **数据集加载与采样**  
   - `s3dis_dataset.py`, `helper_tool.py`  
   - 输入：预处理数据  
   - 输出：采样 batch

3. **数据加载与批处理**  
   - `main_S3DIS.py`, `s3dis_dataset.py`  
   - 输入：采样 batch  
   - 输出：网络输入 batch

4. **模型训练**  
   - `RandLANet.py`, `main_S3DIS.py`  
   - 输入：网络输入 batch  
   - 输出：日志、模型权重

5. **验证与测试**  
   - `test_S3DIS.py`, `RandLANet.py`  
   - 输入：模型权重、测试数据  
   - 输出：预测结果、日志

6. **交叉验证/批量测试（可选）**  
   - `utils/6_fold_cv.py`, `job_for_testing.sh`

---

**每一步都严格依赖前一步的输出数据和相关脚本文件，确保数据流和功能链路完整 。**

更新至2025/08//10  **有待完善**
